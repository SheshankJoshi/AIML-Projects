{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "noticed-corps",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy as sp\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "wired-billy",
   "metadata": {},
   "outputs": [],
   "source": [
    "t1=pd.read_csv(\"phone_user_review_file_1.csv\",low_memory=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "moved-victory",
   "metadata": {},
   "outputs": [],
   "source": [
    "t1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aging-italic",
   "metadata": {},
   "outputs": [],
   "source": [
    "t1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "comic-virginia",
   "metadata": {},
   "outputs": [],
   "source": [
    "t2=pd.read_csv(\"phone_user_review_file_2.csv\",low_memory=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "colonial-bennett",
   "metadata": {},
   "outputs": [],
   "source": [
    "t2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "wrong-impression",
   "metadata": {},
   "outputs": [],
   "source": [
    "t3=pd.read_csv(\"phone_user_review_file_3.csv\",low_memory=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "stunning-brook",
   "metadata": {},
   "outputs": [],
   "source": [
    "t3.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "metric-memorabilia",
   "metadata": {},
   "outputs": [],
   "source": [
    "t4=pd.read_csv(\"phone_user_review_file_4.csv\",low_memory=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "unlikely-jamaica",
   "metadata": {},
   "outputs": [],
   "source": [
    "t4.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "medieval-venezuela",
   "metadata": {},
   "outputs": [],
   "source": [
    "t5=pd.read_csv(\"phone_user_review_file_5.csv\",low_memory=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "handled-acrylic",
   "metadata": {},
   "outputs": [],
   "source": [
    "t5.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "prospective-organ",
   "metadata": {},
   "outputs": [],
   "source": [
    "t6=pd.read_csv(\"phone_user_review_file_6.csv\",low_memory=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "killing-fiction",
   "metadata": {},
   "outputs": [],
   "source": [
    "t6.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "certain-laser",
   "metadata": {},
   "outputs": [],
   "source": [
    "t6.shape[0]+t5.shape[0]+t4.shape[0]+t3.shape[0]+t2.shape[0]+t1.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "greater-triumph",
   "metadata": {},
   "outputs": [],
   "source": [
    "data=pd.concat([t1,t2,t3,t4,t5,t6],ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ranking-colon",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "early-center",
   "metadata": {},
   "source": [
    "Score_max seems to be an irrelevant feature and hence we can drop it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "forward-province",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "developed-criterion",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "capable-azerbaijan",
   "metadata": {},
   "source": [
    "phone_url is also an irrelevant feature so we can go ahead and drop it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "other-exclusive",
   "metadata": {},
   "outputs": [],
   "source": [
    "data[\"lang\"].is_unique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "romantic-fossil",
   "metadata": {},
   "outputs": [],
   "source": [
    "data[\"lang\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "canadian-actor",
   "metadata": {},
   "outputs": [],
   "source": [
    "round(data[\"score\"],1) # Rounding of the decimal places."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "metallic-coral",
   "metadata": {},
   "outputs": [],
   "source": [
    "data[data.isna()].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "honey-edmonton",
   "metadata": {},
   "outputs": [],
   "source": [
    "data[\"domain\"].value_counts() #Notice once invalid character here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "diverse-beaver",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "turned-click",
   "metadata": {},
   "outputs": [],
   "source": [
    "for each in data.columns:\n",
    "    print(each,\" : \",data[each].isna().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "digital-murder",
   "metadata": {},
   "source": [
    "**Some Observations Worth Noting** \n",
    "\n",
    "If score is not given, then that particular entry is completely useless. We can't substitute it with zero, because that would be doing injustice to the product, and some genuine reviews. On top of that the actual review written in words might be completley speaking otherwise. There are other methods which can be used, like analyzing the words used and the user profile to understand if the rating is missing because of some data entry problem or is it some kind of incomplete or fake review that is unsuitable for analysis. But, here we don't have access to NLP kind of analysis, hence we don't opt for it.\n",
    "\n",
    "\n",
    "If author field is not given, it might be irrelevant for a few types of recommendation systems, but for something like apriori or user-user collaborative recommendations it becomes important to identify. Moreoever it becomes really important to understand if the missing entry of the \"author\" is repeated elsewhere. If it is supposedly repeated, meaning that user has bought multiple items and hence, that would cause loss of valuable information regarding the user behaviour.\n",
    "\n",
    "If extract is not given, then that particular entry might be useless if we are going to any kind of recommendation system other than that of popular recommendation settings. So, we have to take a decision to drop these appropriate entries when we are going for content based recommendation systems.\n",
    "\n",
    "There are some expected types of entries into the particular column like for example, \"source\" is expected to be purely a string, but there might be some other entries other than alphabets and some serious characters. We can eliminate them. Country is strictly a string value, but we should check the same, and so that can be used to correct some entries with approximate entries with most matching ones.\n",
    "\n",
    "We can create new columns out of old ones, like date here, which will help us in further analysis.\n",
    "\n",
    "Two entries, Source and the website corresponding to that source both are redundant, hence we can simply drop the source column and retain domain column.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "chinese-government",
   "metadata": {},
   "outputs": [],
   "source": [
    "data[data.duplicated()]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "promising-cowboy",
   "metadata": {},
   "source": [
    "There seems to be two kinds of duplicate entries here, one in which it might mean that review given on one domain might be different than another. Here we are blindly assuming that user doesn't have the same username across the domains, hence two users with same name but different domains will be treated as different users, even though the extract is exactly the same and product is also exactly the same.\n",
    "\n",
    "The other review is the same user, same domain same product but either different rating or different extract. In that case, we have to consider the most recent extract and most recent rating. If there is a collision in terms of date, we have to consider the lowest rating (if there is a rating difference) or choose extract with most number of words (assuming that the user didn't write crappy reviews and the extract actually describes in detail the working of the project.)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "direct-primary",
   "metadata": {},
   "source": [
    "# Proposition for Data Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "meaningful-seven",
   "metadata": {},
   "source": [
    "Step 1. Drop any entries within the rows where the product under review is not mentioned. No matter what the URL is, which can change all the time, the product have to be there for us to take any decision. So, we will try to extract information about the missing product name from the URL. (there seems to be only one such entry here so no problems regarding that).\n",
    "\n",
    "Step 2. Convert the datetime kind of format and check it out. If there are any exceptions while conversions they can be appropriately handled.\n",
    "\n",
    "\n",
    "Step 3. Eliminate Duplicate entries by taking taking author names and trying to find out if they belong to the same domain, if so if they wrote multiple reviews for the same product. If so, check which review is the latest and take that review as the honest one and drop the other. If there is a date conflict, go for the rating, and choose the lowest rating. If the rating is in conflict too, then go for highest number of words in the extract column and choose for that particular entry and drop the others. We are going to exclusively deal with index vlaues here, so we have to be careful.\n",
    "\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "reduced-taste",
   "metadata": {},
   "source": [
    "**Step 1**    : Checking for product columns missing values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "focused-grain",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Checking for Product NaN columns\n",
    "temp=data[\"product\"][data[\"product\"].isna()].index\n",
    "data.loc[temp]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fancy-husband",
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_url=data.loc[temp][\"phone_url\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "unauthorized-roulette",
   "metadata": {},
   "outputs": [],
   "source": [
    "for each_item in data[\"phone_url\"]:\n",
    "    if type(each_item)!=str:\n",
    "        print(each_item)\n",
    "#Looks like there is no columns which is not recognized as a string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "brown-marina",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "noble-carroll",
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "boxed-commodity",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "same_url_list=[]\n",
    "for each_ind in data.index:\n",
    "    k=re.search('/*/samsung-galaxy-s-iii/',data[\"phone_url\"][each_ind])\n",
    "    if k:\n",
    "        same_url_list.append(each_ind)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "minor-lexington",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.loc[same_url_list].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "heated-vertical",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.loc[same_url_list].count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fifty-bidding",
   "metadata": {},
   "source": [
    "Looks like we can substitute one of the enties into the product column since it looks perfectly fine except the author missing column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "subject-council",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now let us replace the product column of that particular entry in the dataframe with approrpoiate mechanism.\n",
    "\n",
    "data.loc[temp][\"product\"]=data.loc[same_url_list].head().iloc[1][\"product\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "functional-designer",
   "metadata": {},
   "source": [
    "We can generalize this whole thing and go ahead, but let us restrict ourselves to this particular case where I randomly picked the product name. **Assuming that there are no vairations for within the product.** . Hopefully that will stay the way, even though we have grabbed it from different domain and different user. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "designed-democrat",
   "metadata": {},
   "source": [
    "**Step 2**   : Converting to the Date-Time Timeline for analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "mobile-drinking",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Converting the Date columns to a much more feasilbe format.\n",
    "data[\"date\"]=pd.to_datetime(data[\"date\"], infer_datetime_format=True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "indie-modem",
   "metadata": {},
   "outputs": [],
   "source": [
    "data[\"date\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "working-minimum",
   "metadata": {},
   "outputs": [],
   "source": [
    "data[\"date\"].loc[1415128].month\n",
    "#Small demonstration of the effect."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "local-aquatic",
   "metadata": {},
   "outputs": [],
   "source": [
    "duplicate_entries=data[data.duplicated(subset=[\"author\",\"product\"],keep=False)].index\n",
    "data.loc[duplicate_entries]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "natural-duration",
   "metadata": {},
   "outputs": [],
   "source": [
    "duplicate_entries_df=data.loc[duplicate_entries]\n",
    "dom=duplicate_entries_df[duplicate_entries_df[\"domain\"]==\"amazon.in\"]\n",
    "dom"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "corporate-vancouver",
   "metadata": {},
   "outputs": [],
   "source": [
    "duplicate_entries_df.groupby(\"domain\").get_group(\"amazon.com\")[duplicate_entries_df.groupby(\"domain\").get_group(\"amazon.com\").duplicated(subset=[\"author\"])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "front-filling",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "widespread-crowd",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(seed=612) # Setting random state for seeding as given.\n",
    "random_indices=np.random.randint(0,data.shape[0],1000000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "driven-judge",
   "metadata": {},
   "outputs": [],
   "source": [
    "random_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ultimate-grain",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_stage3=data.loc[random_indices].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "expected-stuff",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_stage3.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "oriented-sessions",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_stage3[\"score_max\"].value_counts() #Looks like all of them have a maximum score of 10, so it is irrelevant feature for now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "executed-array",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_stage3.drop([\"source\",\"phone_url\",\"score_max\"],axis=1,inplace=True) #dropping irrelevant features for now."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "original-links",
   "metadata": {},
   "source": [
    "To identify most rated features or the phones with most rating, we should have it such that the phones or entries with no rating can't be considered i.e. missing rating can be and should be eliminated and they can't be be imputed with anything else, because that would beat the purpose of it all. It pushes unncessary entries into the recommendations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "applied-lesbian",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_stage3.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "enclosed-austria",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_stage3[data_stage3[\"score\"].isna()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "brilliant-discount",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_stage3[\"score\"][data_stage3['score'].notna()].count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "roman-twist",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_stage3=data_stage3[data_stage3['score'].notna()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "superb-buffalo",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_stage3[\"score\"]=data_stage3[\"score\"].round().astype(\"int64\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "robust-disability",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_stage3[\"product\"][data_stage3[\"score\"]==data_stage3[\"score\"].max()]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "shaped-conclusion",
   "metadata": {},
   "source": [
    "These are the top phones that have got the maximum rating. But, that alone is not the criteria for phone recommendations here. For being a top recommendation being popular, it should have two characteristics :\n",
    "\n",
    "1. Be top rated most of the times i.e. mean rating for the phone should be highest of all. \n",
    "\n",
    "2. It should be highest occurring of all i.e. it should be extremely frequent within the count.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "focal-chester",
   "metadata": {},
   "source": [
    "**Please Note** : \n",
    "\n",
    "Here two phones with similar features but a little different variations (e.g. different color), are considered as different products. In other words, it is assumed that those variations (colors) within the features are assumed to be totally different features in themselves. It requires huge amount of processing power and also some deep analytics of NLP to deal with similarity between two different phone version. We will try to attempt them but for now, we will consider those products are different products. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fallen-calcium",
   "metadata": {},
   "outputs": [],
   "source": [
    "value_count_products=data_stage3[\"product\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "direct-classification",
   "metadata": {},
   "outputs": [],
   "source": [
    "value_count_products.keys() # Having a breif look at the most sold phones, and total variety of phones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "blessed-centre",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_stage3_score_grouping=data_stage3.groupby(\"score\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "disturbed-following",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_stage3_score_grouping.indices #Checking out the groupings of which phones have beeen rated how. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hindu-theta",
   "metadata": {},
   "source": [
    "Noting from the above the indices of the particular phones whose indices and the particular ratings are given. Now, let us create another column where the mean score for that product is attributed. But before that we have to group the phones by their type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "essential-stocks",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_stage3_product_grouping=data_stage3.groupby(\"product\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "distinct-nowhere",
   "metadata": {},
   "source": [
    "It is to be noted that phones that are sold should be sold by a minimum amount of the total phones sold to be even eligible for consideration of the top recommendations. Hence, we will eliminate all the phones that are sold just one or two in quantity and consider the ratings for the most sold phones and then sort those phones in accordance to their mean rating.\n",
    "\n",
    "I have set here threshold to be 10% of the most sold phone's frequency (the total sales to be cut off and hence has been noted down.) from the very strong assumption that most frequently occurring product is the product that is most sold, which might be extremely wrong assumption.( I myself haven't rated a lot of items I bought)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "speaking-filling",
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold_phones=value_count_products[0]/10 # This threshold value can be changed.\n",
    "ratings_phones={}\n",
    "######################################################################\n",
    "for each_phone in value_count_products.keys():\n",
    "    #print(each_phone)\n",
    "    if value_count_products[each_phone]<threshold_phones:\n",
    "        #print(value_count_products[each_phone])\n",
    "        #print(each_phone)\n",
    "        continue\n",
    "    else:\n",
    "        #print(each_phone)\n",
    "        #print(data_stage3_product_grouping.get_group(each_phone)[\"score\"].mean())\n",
    "        ratings_phones.update({each_phone: data_stage3_product_grouping.get_group(each_phone)[\"score\"].mean()})\n",
    "####################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "growing-company",
   "metadata": {},
   "outputs": [],
   "source": [
    " # Sorting the values so that recommendations come up automatically.\n",
    "####################################################################\n",
    "# Function that sorts the dictionary based on the values\n",
    "####################################################################\n",
    "def sort_dict_values(given_dict,reverse=True):\n",
    "    sorted_values=sorted(given_dict.values(),reverse=reverse)\n",
    "    sorted_ratings={}\n",
    "    for i in sorted_values:\n",
    "        for k in given_dict.keys():\n",
    "            if given_dict[k] == i:\n",
    "                sorted_ratings[k]=given_dict[k]\n",
    "                break\n",
    "    return sorted_ratings\n",
    "###############################################################\n",
    "# End of funtion\n",
    "###############################################################\n",
    "sorted_ratings_phones = sort_dict_values(ratings_phones)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "balanced-catholic",
   "metadata": {},
   "source": [
    "The above cell is copied from the internet at address given below :\n",
    "https://stackabuse.com/how-to-sort-dictionary-by-value-in-python/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "binary-twins",
   "metadata": {},
   "outputs": [],
   "source": [
    "###########################################################################\n",
    "# Gives top phones based on the recommendations and the threshold as set before.\n",
    "###########################################################################\n",
    "def give_top_phones(sorted_ratings_phones,how_many_phones=10,min_rating=None):\n",
    "    phones=[]\n",
    "    if min_rating!=None:\n",
    "        for each_item in sorted_ratings_phones:\n",
    "            if sorted_ratings_phones[each_item]>=min_rating:\n",
    "                phones.append([each_item,sorted_ratings_phones[each_item]])\n",
    "        #print(phones)\n",
    "        phones=pd.DataFrame(phones,columns=[\"Product\",\"Avg_Rating\"])\n",
    "    else:\n",
    "        phones=pd.DataFrame(phones,columns=[\"Product\",\"Avg_Rating\"])\n",
    "    return phones[:how_many_phones]#pd.DataFrame(phones,columns=[\"Phone\",\"Rating\"])\n",
    "###########################################################################\n",
    "# End of function\n",
    "###########################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "received-vietnam",
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_ratings_phones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "affected-yacht",
   "metadata": {},
   "outputs": [],
   "source": [
    "give_top_phones(sorted_ratings_phones,how_many_phones=30,min_rating=8) # You can change the rating and the number of phones here."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "overhead-difficulty",
   "metadata": {},
   "source": [
    "This is the complete recommendation based on average rating (mean rating) for devices based on certain threshold conditions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "convertible-gibraltar",
   "metadata": {},
   "outputs": [],
   "source": [
    "data[\"author\"].value_counts(dropna=False)[:15]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "above-contest",
   "metadata": {},
   "source": [
    "It is completely impractical for a person to rate those many reviews i.e. 76978. It is clear that those particular customers failed to provide their names. They can be the same author or different author. There are hundreds of possibilities here too. Within it, there are places where the author's name is not mentioned (NaN) values are there implying that those customers simply refused to mention their names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "found-constraint",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_stage4=data_stage3.copy()\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "innovative-substitute",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_stage4['product'] = data_stage4['product'].map(lambda x: x.split(','))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eastern-wonder",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_stage4[\"product\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "pregnant-shield",
   "metadata": {},
   "outputs": [],
   "source": [
    "#from rake_nltk import Rake"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "current-opening",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Checking for keywords and most important features.\n",
    "#for index, row in data_stage4.iterrows():\n",
    "#    features = row['product']\n",
    "#    r = Rake()\n",
    "#    r.extract_keywords_from_text(features)\n",
    "#    key_words_dict_scores = r.get_word_degrees()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "lovely-pennsylvania",
   "metadata": {},
   "outputs": [],
   "source": [
    "count_df=pd.DataFrame(sorted_ratings_phones,index=range(len(sorted_ratings_phones)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "wound-sweet",
   "metadata": {},
   "outputs": [],
   "source": [
    "count_df.columns.str.split(\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "brave-registration",
   "metadata": {},
   "outputs": [],
   "source": [
    "count_input=[]\n",
    "for entry in count_df.columns.str.split(\",\"):\n",
    "    for each_entry in entry:\n",
    "        count_input.append(each_entry)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "northern-december",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.Series(count_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hearing-surgery",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.Series(count_input).value_counts()[:10] # this shows the most important features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "liquid-scanning",
   "metadata": {},
   "source": [
    "**Observations** :  16 GB phones seem to be very popular and required of the important features here. Also, phones with Fotocamera (might be other language used here) might be the most important feature, but along with it 4G seem to be most important feature here.\n",
    "\n",
    "But it needs a little more detail into what are the proper details on the most important features. It needs count vectorizer for each "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "enhanced-precipitation",
   "metadata": {},
   "outputs": [],
   "source": [
    "count = CountVectorizer()\n",
    "count_matrix = count.fit_transform(pd.Series(count_input))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sized-voice",
   "metadata": {},
   "outputs": [],
   "source": [
    "count_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "defined-response",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(sorted_ratings_phones,index=range(len(sorted_ratings_phones))).columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "collective-reader",
   "metadata": {},
   "outputs": [],
   "source": [
    "count_matrix=count_matrix.todense()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hundred-effort",
   "metadata": {},
   "outputs": [],
   "source": [
    "count_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cleared-purple",
   "metadata": {},
   "outputs": [],
   "source": [
    "count.vocabulary_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "intimate-characteristic",
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_features=sort_dict_values(count.vocabulary_,reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "italic-certificate",
   "metadata": {},
   "outputs": [],
   "source": [
    "how_many_features=20\n",
    "########################################################################\n",
    "#\n",
    "#######################################################################\n",
    "\n",
    "def give_top_features(sorted_features,how_many_features=10):\n",
    "    phones=[]\n",
    "    phones=pd.DataFrame(pd.Series(sorted_features),columns=[\"Popularity\"])\n",
    "    return phones[:how_many_features]#pd.DataFrame(phones,columns=[\"Phone\",\"Rating\"])\n",
    "########################################################################\n",
    "#\n",
    "########################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "delayed-butterfly",
   "metadata": {},
   "outputs": [],
   "source": [
    "give_top_features(sorted_features,how_many_features=how_many_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "appreciated-hungarian",
   "metadata": {},
   "source": [
    "Here are the most popular features of the phone. Since some of them are not in english langauge and we did some random sampling, we might not be able to figure it out.\n",
    "\n",
    "It is also to note that some of them are brands, some of them are just plain features, and some of them are phone model. Here the assumption and distinction between them is not made at all."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "extraordinary-education",
   "metadata": {},
   "outputs": [],
   "source": [
    "#count2 = CountVectorizer()\n",
    "#count_matrix2 = count2.fit_transform(data_stage4[\"product\"].values)\n",
    "\n",
    "#sorted_features2=sort_dict_values(count2.vocabulary_,reverse=True)\n",
    "\n",
    "#give_top_features(sorted_features2,how_many_features=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "proved-swaziland",
   "metadata": {},
   "source": [
    "A product should be mentioned atleast 50 times for it to be eligible to be separated according to the given question. From those ones, the things that are to be rated atleast 50 times should be separated out. So, first let us create the index of items which have been mentioned at least 50 times."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "according-continent",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_stage4[\"product\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "compatible-rehabilitation",
   "metadata": {},
   "outputs": [],
   "source": [
    "temp=data_stage4[\"product\"].value_counts()[data_stage4[\"product\"].value_counts()>50].index\n",
    "#\n",
    "indices_to_keep=[] # this is the list of indices where there are atleast 50 repetitions for the product\n",
    "###########################################################################\n",
    "for each in list(temp):\n",
    "    for indices in data_stage4[data_stage4[\"product\"]==each].index:\n",
    "        indices_to_keep.append(indices)\n",
    "###########################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "excessive-channels",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_stage5=data_stage4.loc[indices_to_keep].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sticky-sweet",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_stage5[\"author\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "manufactured-watershed",
   "metadata": {},
   "outputs": [],
   "source": [
    "temp=data_stage5[\"author\"].value_counts()[data_stage5[\"author\"].value_counts()>50]\n",
    "#\n",
    "indices_to_keep=[] # this is the list of indices where there are atleast 50 reviews for the given user.\n",
    "###########################################################################\n",
    "for each in list(temp):\n",
    "    for indices in list(data_stage5[data_stage5[\"author\"]==each].index):\n",
    "        indices_to_keep.append(indices)\n",
    "###########################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "advance-establishment",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_stage5=data_stage5.loc[indices_to_keep].copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "imposed-african",
   "metadata": {},
   "source": [
    "This seem to include features or more like names that are specialized and has been mentioned multiple times. It doesn't matter much more than that. But, if we take into account here, then we have isolated the products "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
