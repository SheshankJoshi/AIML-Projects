{
  "nbformat": 4,
  "nbformat_minor": 5,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.11"
    },
    "colab": {
      "name": "Part 1 - IMDB Sentiment Classifier - Chapter 2.ipynb",
      "provenance": [],
      "collapsed_sections": []
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "46759e53"
      },
      "source": [
        "# Chapter 2"
      ],
      "id": "46759e53"
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LK1_3-0JpDBs",
        "outputId": "346da98d-25b0-45fe-ede9-9f652f9183fa"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "id": "LK1_3-0JpDBs",
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5bFP31UIo80B",
        "outputId": "e62e8107-d593-467a-f825-b7f348ff4716"
      },
      "source": [
        "!pip install nltk\n",
        "!pip install pandas\n",
        "!pip install tensorflow"
      ],
      "id": "5bFP31UIo80B",
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.7/dist-packages (3.2.5)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from nltk) (1.15.0)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (1.1.5)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: numpy>=1.15.4 in /usr/local/lib/python3.7/dist-packages (from pandas) (1.19.5)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas) (2018.9)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas) (1.15.0)\n",
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.7/dist-packages (2.5.0)\n",
            "Requirement already satisfied: gast==0.4.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (0.4.0)\n",
            "Requirement already satisfied: absl-py~=0.10 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (0.12.0)\n",
            "Requirement already satisfied: numpy~=1.19.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (1.19.5)\n",
            "Requirement already satisfied: h5py~=3.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (3.1.0)\n",
            "Requirement already satisfied: keras-preprocessing~=1.1.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (1.1.2)\n",
            "Requirement already satisfied: flatbuffers~=1.12.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (1.12)\n",
            "Requirement already satisfied: termcolor~=1.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (1.1.0)\n",
            "Requirement already satisfied: opt-einsum~=3.3.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (3.3.0)\n",
            "Requirement already satisfied: six~=1.15.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (1.15.0)\n",
            "Requirement already satisfied: typing-extensions~=3.7.4 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (3.7.4.3)\n",
            "Requirement already satisfied: tensorflow-estimator<2.6.0,>=2.5.0rc0 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (2.5.0)\n",
            "Requirement already satisfied: keras-nightly~=2.5.0.dev in /usr/local/lib/python3.7/dist-packages (from tensorflow) (2.5.0.dev2021032900)\n",
            "Requirement already satisfied: google-pasta~=0.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: grpcio~=1.34.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (1.34.1)\n",
            "Requirement already satisfied: protobuf>=3.9.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (3.17.3)\n",
            "Requirement already satisfied: tensorboard~=2.5 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (2.5.0)\n",
            "Requirement already satisfied: wrapt~=1.12.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (1.12.1)\n",
            "Requirement already satisfied: wheel~=0.35 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (0.37.0)\n",
            "Requirement already satisfied: astunparse~=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (1.6.3)\n",
            "Requirement already satisfied: cached-property in /usr/local/lib/python3.7/dist-packages (from h5py~=3.1.0->tensorflow) (1.5.2)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.5->tensorflow) (0.4.5)\n",
            "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.5->tensorflow) (0.6.1)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.5->tensorflow) (1.8.0)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.5->tensorflow) (1.0.1)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.5->tensorflow) (3.3.4)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.5->tensorflow) (2.23.0)\n",
            "Requirement already satisfied: google-auth<2,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.5->tensorflow) (1.34.0)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.5->tensorflow) (57.2.0)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard~=2.5->tensorflow) (4.2.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard~=2.5->tensorflow) (0.2.8)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard~=2.5->tensorflow) (4.7.2)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.5->tensorflow) (1.3.0)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard~=2.5->tensorflow) (4.6.3)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<2,>=1.6.3->tensorboard~=2.5->tensorflow) (0.4.8)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.5->tensorflow) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.5->tensorflow) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.5->tensorflow) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.5->tensorflow) (2021.5.30)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.5->tensorflow) (3.1.1)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->markdown>=2.6.8->tensorboard~=2.5->tensorflow) (3.5.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ti3n5sqBtL63"
      },
      "source": [
        "path=\"/content/drive/MyDrive/Colab Notebooks\""
      ],
      "id": "ti3n5sqBtL63",
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Un-7iMf_tY1-"
      },
      "source": [
        "import os\n",
        "os.chdir(path)"
      ],
      "id": "Un-7iMf_tY1-",
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "17fdbf85"
      },
      "source": [
        "import pandas as pd"
      ],
      "id": "17fdbf85",
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "60764297"
      },
      "source": [
        "X_train_w=pd.read_csv(\"X_train_Imdb.csv\",index_col=False)\n",
        "X_test_w=pd.read_csv(\"X_test_Imdb.csv\",index_col=False)\n",
        "y_train=pd.read_csv(\"y_train_Imdb.csv\",index_col=False)\n",
        "y_test=pd.read_csv(\"y_test_Imdb.csv\",index_col=False)\n",
        "X_train_d=pd.read_csv(\"X_train_coded_Imdb.csv\",index_col=False)\n",
        "X_test_d=pd.read_csv(\"X_test_coded_Imdb.csv\",index_col=False)"
      ],
      "id": "60764297",
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4eeade27",
        "outputId": "9e2f4c63-580e-404f-9ec7-a873c19656f6"
      },
      "source": [
        "import nltk\n",
        "import re\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.corpus import stopwords\n",
        "nltk.download(\"stopwords\")\n",
        "nltk.download(\"wordnet\")\n",
        "stop_words = stopwords.words(\"english\")\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "\n",
        "# Found this handy function online but also lost the source where I found it. I slightly modified it to remove the padding automatically.\n",
        "\n",
        "################################################################3\n",
        "# Handy function for lemmatizing and cleaning the text\n",
        "#################################################################\n",
        "\n",
        "def clean_text(text):\n",
        "    #text=text[]\n",
        "    text = re.sub(r'[^\\w\\s]','',text, re.UNICODE)\n",
        "    text = text.lower()\n",
        "    text = [lemmatizer.lemmatize(token) for token in text.split(\" \")]\n",
        "    text = [lemmatizer.lemmatize(token, \"v\") for token in text]\n",
        "    text = [word for word in text if not word in stop_words]\n",
        "    text = \" \".join(text)\n",
        "    return text\n",
        "\n",
        "################################################################3\n",
        "# End of Function\n",
        "#################################################################"
      ],
      "id": "4eeade27",
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/wordnet.zip.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e443d5df"
      },
      "source": [
        "#X_train_w[\"0\"]"
      ],
      "id": "e443d5df",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1f45d378"
      },
      "source": [
        "#X_train_w[\"0\"].apply(clean_text)"
      ],
      "id": "1f45d378",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "628ca227"
      },
      "source": [
        "X_train_w=X_train_w[\"0\"].apply(clean_text)\n",
        "X_test_w=X_test_w[\"0\"].apply(clean_text)"
      ],
      "id": "628ca227",
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ec16ec1e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1be77914-5c08-4b14-c5cf-25c0bfbc5c65"
      },
      "source": [
        "X_train_w.to_numpy()"
      ],
      "id": "ec16ec1e",
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array(['scream able filmmaker series see 10 part br screen sure wa one life always life good huge look brilliant wa cinema decide major history br alone performance never model slut major saw film funny pretty person',\n",
              "       'wa doe accent br look give wang weird win lee superb br screen hang write borrow self wa play else rule 1997 modern would 1996 play version script le br hear good spoiler late consider slasher think sense build sword davis b dog film beach br fair view',\n",
              "       'arent badly ha werent bruce like brave dreadful find seem accidentally le good br excellent instead around guy time somewhat unwatchable love france smallest look make enjoy rest perhaps badly play wife consist downright get',\n",
              "       ...,\n",
              "       'wa one point mostly completely br ariel act hot wa say ensemble experience wa sexual episode wa due br tell ill br fast always life see cast run x br one roommate opinion may point pop lot br american ever',\n",
              "       'funny child spoil fan element roll instead vance material good insomnia alright andy p short pull stun wa rather completely br besides wa one appear often entire movie show character movie note capture see',\n",
              "       'wa br mind kick movie britain pure ultimately truth would ask stake burst robin 24 woman ashley notice little br say also cabin richards make choice entirely yet br entirely think fun story one goofy movie killer police commit find like part cannot everyone would hear whole could state series woman'],\n",
              "      dtype=object)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "894a36f9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "728a75b6-5dc1-4617-fa10-11df941df590"
      },
      "source": [
        "tokenizer_training=pd.concat([X_train_w,X_test_w],ignore_index=True)\n",
        "tokenizer_training\n"
      ],
      "id": "894a36f9",
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0       scream able filmmaker series see 10 part br sc...\n",
              "1       wa doe accent br look give wang weird win lee ...\n",
              "2       arent badly ha werent bruce like brave dreadfu...\n",
              "3       might sexuality since find even actor weird ke...\n",
              "4       wa least new complain church dvd ha give scale...\n",
              "                              ...                        \n",
              "5731    poor chase good decency usual upon surely stuf...\n",
              "5732    give ha never lie movie kid br enough always l...\n",
              "5733    joke ridiculous leader br headache unfair movi...\n",
              "5734    wa body character later wa puppet character wa...\n",
              "5735    ive angst candidate deliver get work number ne...\n",
              "Name: 0, Length: 5736, dtype: object"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "e234deea"
      },
      "source": [
        "from keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Embedding, LSTM, SpatialDropout1D\n",
        "vocabSize=10000 #We are using only most frequent 10000 words.\n",
        "embed_dim = 64\n",
        "lstm_out = 32\n",
        "embed_size=128 # To Change\n",
        "max_features = 6000\n",
        "#\n",
        "tokenizer = Tokenizer(num_words=max_features)\n",
        "tokenizer.fit_on_texts(tokenizer_training)"
      ],
      "id": "e234deea",
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f1a906eb"
      },
      "source": [
        "# splitting our test set into testing and validation pieces\n",
        "X_valid,X_test=X_test_w[:350],X_test_w[350:]\n",
        "y_valid,y_test=y_test[:350],y_test[350:]"
      ],
      "id": "f1a906eb",
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d1e0805b"
      },
      "source": [
        "X_train = tokenizer.texts_to_sequences(X_train_w)\n",
        "X_valid = tokenizer.texts_to_sequences(X_valid)\n",
        "X_test = tokenizer.texts_to_sequences(X_test)\n",
        "# We will tokenize the test set later on\n",
        "#\n",
        "maxlen = 130"
      ],
      "id": "d1e0805b",
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "70942878"
      },
      "source": [
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "maxlen = 30\n",
        "X_train = pad_sequences(X_train,maxlen=maxlen)\n",
        "X_valid = pad_sequences(X_valid,maxlen=maxlen)\n",
        "X_test = pad_sequences(X_test,maxlen=maxlen)"
      ],
      "id": "70942878",
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "48RMQvqoywDO"
      },
      "source": [
        "y_test=y_test[\"0\"]\n",
        "y_train=y_train[\"0\"]\n",
        "y_valid=y_valid[\"0\"]"
      ],
      "id": "48RMQvqoywDO",
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0c996b2d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e1ee802e-1a97-4abc-a417-cd1b4e2654ac"
      },
      "source": [
        "X_train.shape,X_test.shape,X_valid.shape\n"
      ],
      "id": "0c996b2d",
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((2963, 30), (2423, 30), (350, 30))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ab2959b4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9b8031b6-3f8a-4994-8fc0-199e7e617f4d"
      },
      "source": [
        "from keras.layers import Dense , Input , Dropout , Activation, GRU, Flatten\n",
        "from keras.layers import Bidirectional, GlobalMaxPool1D\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Embedding(max_features, embed_size))\n",
        "model.add(SpatialDropout1D(0.2))\n",
        "model.add(LSTM(64, return_sequences = True))\n",
        "#model.add(Bidirectional(LSTM(64, return_sequences = True)))\n",
        "model.add(LSTM(32, return_sequences = True))\n",
        "model.add(GlobalMaxPool1D())\n",
        "model.add(Dense(20, activation=\"relu\"))\n",
        "model.add(Dropout(0.05))\n",
        "model.add(Dense(1, activation=\"softmax\"))\n",
        "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "model.summary()"
      ],
      "id": "ab2959b4",
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_2\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_2 (Embedding)      (None, None, 128)         768000    \n",
            "_________________________________________________________________\n",
            "spatial_dropout1d_1 (Spatial (None, None, 128)         0         \n",
            "_________________________________________________________________\n",
            "lstm_2 (LSTM)                (None, None, 64)          49408     \n",
            "_________________________________________________________________\n",
            "lstm_3 (LSTM)                (None, None, 32)          12416     \n",
            "_________________________________________________________________\n",
            "module_wrapper (ModuleWrappe (None, 32)                0         \n",
            "_________________________________________________________________\n",
            "module_wrapper_1 (ModuleWrap (None, 20)                660       \n",
            "_________________________________________________________________\n",
            "module_wrapper_2 (ModuleWrap (None, 20)                0         \n",
            "_________________________________________________________________\n",
            "module_wrapper_3 (ModuleWrap (None, 1)                 21        \n",
            "=================================================================\n",
            "Total params: 830,505\n",
            "Trainable params: 830,505\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GY8YEhDdzVog"
      },
      "source": [
        "**Please Note**\n",
        "I tried my best to include Bidirectional Layer. But it is causing issue with higher versions, incompatible with lower versions and is causing issues with everything. I couldn't don't anything about it. "
      ],
      "id": "GY8YEhDdzVog"
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fgRuWUfZyDSk",
        "outputId": "330bf5f7-7d06-4abc-8ceb-a4ef6bf4bb16"
      },
      "source": [
        "batch_size = 60\n",
        "epochs = 25\n",
        "model.fit(X_train,y_train, batch_size=batch_size, epochs=epochs, validation_data=(X_valid,y_valid))"
      ],
      "id": "fgRuWUfZyDSk",
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/25\n",
            "50/50 [==============================] - 4s 75ms/step - loss: 0.0378 - accuracy: 0.5413 - val_loss: 1.0161 - val_accuracy: 0.5771\n",
            "Epoch 2/25\n",
            "50/50 [==============================] - 3s 58ms/step - loss: 0.0182 - accuracy: 0.5413 - val_loss: 1.1694 - val_accuracy: 0.5771\n",
            "Epoch 3/25\n",
            "50/50 [==============================] - 3s 58ms/step - loss: 0.0127 - accuracy: 0.5413 - val_loss: 1.2658 - val_accuracy: 0.5771\n",
            "Epoch 4/25\n",
            "50/50 [==============================] - 3s 58ms/step - loss: 0.0092 - accuracy: 0.5413 - val_loss: 1.4458 - val_accuracy: 0.5771\n",
            "Epoch 5/25\n",
            "50/50 [==============================] - 3s 58ms/step - loss: 0.0069 - accuracy: 0.5413 - val_loss: 1.5586 - val_accuracy: 0.5771\n",
            "Epoch 6/25\n",
            "50/50 [==============================] - 3s 57ms/step - loss: 0.0073 - accuracy: 0.5413 - val_loss: 1.6136 - val_accuracy: 0.5771\n",
            "Epoch 7/25\n",
            "50/50 [==============================] - 3s 58ms/step - loss: 0.0060 - accuracy: 0.5413 - val_loss: 1.6408 - val_accuracy: 0.5771\n",
            "Epoch 8/25\n",
            "50/50 [==============================] - 3s 58ms/step - loss: 0.0067 - accuracy: 0.5413 - val_loss: 1.6893 - val_accuracy: 0.5771\n",
            "Epoch 9/25\n",
            "50/50 [==============================] - 3s 57ms/step - loss: 0.0083 - accuracy: 0.5413 - val_loss: 1.3269 - val_accuracy: 0.5771\n",
            "Epoch 10/25\n",
            "50/50 [==============================] - 3s 58ms/step - loss: 0.0273 - accuracy: 0.5413 - val_loss: 1.1583 - val_accuracy: 0.5771\n",
            "Epoch 11/25\n",
            "50/50 [==============================] - 3s 58ms/step - loss: 0.0385 - accuracy: 0.5413 - val_loss: 1.0379 - val_accuracy: 0.5771\n",
            "Epoch 12/25\n",
            "50/50 [==============================] - 3s 57ms/step - loss: 0.0234 - accuracy: 0.5413 - val_loss: 1.2271 - val_accuracy: 0.5771\n",
            "Epoch 13/25\n",
            "50/50 [==============================] - 3s 57ms/step - loss: 0.0194 - accuracy: 0.5413 - val_loss: 1.3594 - val_accuracy: 0.5771\n",
            "Epoch 14/25\n",
            "50/50 [==============================] - 3s 58ms/step - loss: 0.0127 - accuracy: 0.5413 - val_loss: 1.5582 - val_accuracy: 0.5771\n",
            "Epoch 15/25\n",
            "50/50 [==============================] - 3s 58ms/step - loss: 0.0109 - accuracy: 0.5413 - val_loss: 1.5877 - val_accuracy: 0.5771\n",
            "Epoch 16/25\n",
            "50/50 [==============================] - 3s 57ms/step - loss: 0.0085 - accuracy: 0.5413 - val_loss: 1.5382 - val_accuracy: 0.5771\n",
            "Epoch 17/25\n",
            "50/50 [==============================] - 3s 57ms/step - loss: 0.0080 - accuracy: 0.5413 - val_loss: 1.5080 - val_accuracy: 0.5771\n",
            "Epoch 18/25\n",
            "50/50 [==============================] - 3s 58ms/step - loss: 0.0064 - accuracy: 0.5413 - val_loss: 1.6084 - val_accuracy: 0.5771\n",
            "Epoch 19/25\n",
            "50/50 [==============================] - 3s 59ms/step - loss: 0.0043 - accuracy: 0.5413 - val_loss: 1.6664 - val_accuracy: 0.5771\n",
            "Epoch 20/25\n",
            "50/50 [==============================] - 3s 60ms/step - loss: 0.0035 - accuracy: 0.5413 - val_loss: 1.6989 - val_accuracy: 0.5771\n",
            "Epoch 21/25\n",
            "50/50 [==============================] - 3s 59ms/step - loss: 0.0036 - accuracy: 0.5413 - val_loss: 1.7457 - val_accuracy: 0.5771\n",
            "Epoch 22/25\n",
            "50/50 [==============================] - 3s 58ms/step - loss: 0.0035 - accuracy: 0.5413 - val_loss: 1.7709 - val_accuracy: 0.5771\n",
            "Epoch 23/25\n",
            "50/50 [==============================] - 3s 58ms/step - loss: 0.0035 - accuracy: 0.5413 - val_loss: 1.7905 - val_accuracy: 0.5771\n",
            "Epoch 24/25\n",
            "50/50 [==============================] - 3s 58ms/step - loss: 0.0028 - accuracy: 0.5413 - val_loss: 1.8164 - val_accuracy: 0.5771\n",
            "Epoch 25/25\n",
            "50/50 [==============================] - 3s 57ms/step - loss: 0.0035 - accuracy: 0.5413 - val_loss: 1.8493 - val_accuracy: 0.5771\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7fad4e25a210>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ckb09y67ylE0"
      },
      "source": [
        "y_pred=model.predict(X_test)"
      ],
      "id": "ckb09y67ylE0",
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4EwVwm59zst_",
        "outputId": "0cce8b4d-9a4c-48b8-ff1e-660ede48d5b0"
      },
      "source": [
        "from sklearn.metrics import classification_report\n",
        "\n",
        "print(classification_report(y_test,y_pred))"
      ],
      "id": "4EwVwm59zst_",
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.00      0.00      0.00      1088\n",
            "           1       0.55      1.00      0.71      1335\n",
            "\n",
            "    accuracy                           0.55      2423\n",
            "   macro avg       0.28      0.50      0.36      2423\n",
            "weighted avg       0.30      0.55      0.39      2423\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6qC6dTp1z-X2"
      },
      "source": [
        ""
      ],
      "id": "6qC6dTp1z-X2",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hjRexze00GLX"
      },
      "source": [
        "**Final Conclusions**\n",
        "\n",
        "We can definitely improve this model, with proper test train split to avoid any Bias. But, the errors and debugging took so much time that I am finally submitting only the working version of my project.\n",
        "\n",
        "Sentiments identifying is a tough job. Especially since a lot of these seem to have some kind of sarcastic tone, sometimes they aren't even properly placed. All the appropriate steps have been undertaken and addressed with here. In future, when all the errors and incompatibilities have been eliminated, we will probably see a lot more clear version with this."
      ],
      "id": "hjRexze00GLX"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fvgFZ68d0mqk"
      },
      "source": [
        ""
      ],
      "id": "fvgFZ68d0mqk",
      "execution_count": null,
      "outputs": []
    }
  ]
}