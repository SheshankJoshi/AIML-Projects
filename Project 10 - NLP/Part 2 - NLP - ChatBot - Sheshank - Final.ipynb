{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c14ff6cc",
   "metadata": {},
   "source": [
    "**Please Note**\n",
    "\n",
    "This is working model but not perfectly working. It has its shortcoming, but it is based on the link given in the PDF of the problem statement. It required some modifications and that's it. Though it is working, because of time complications it hasn't been completely tested thoroughly.\n",
    "\n",
    "I tried doing it with Rasa. But couldn't finish it in time. Also the idea is to include the working."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "da0cb109",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open(\"C:\\\\Users\\\\Sheshank_Joshi\\\\Dropbox\\\\code\\\\jupyter-AIML\\\\Projects\\\\Project 10 - NLP\\\\GL Bot.json\") as file:\n",
    "    Corpus=json.load(file)\n",
    "#print(Corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6d410a1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.chat.util import Chat, reflections\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import word_tokenize\n",
    "import string\n",
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "stop_words = stopwords.words('english') + list(string.punctuation)\n",
    "stemmer = PorterStemmer()\n",
    "lemmatizer=WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b40058c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "W=[]\n",
    "L=[]\n",
    "doc_x=[]\n",
    "doc_y=[]\n",
    "\n",
    "for intent in Corpus[\"intents\"]:\n",
    "    for pattern in intent[\"patterns\"]:\n",
    "        w_temp=word_tokenize(pattern)\n",
    "        #print(w)\n",
    "        W.extend(w_temp)\n",
    "        doc_x.append(w_temp)\n",
    "        doc_y.append(intent[\"tag\"])\n",
    "    if intent[\"tag\"] not in L:\n",
    "        L.append(intent[\"tag\"])\n",
    "for each_word in W:\n",
    "    if each_word in stop_words:\n",
    "        W.remove(each_word)\n",
    "W=[stemmer.stem(w.lower()) for w in W if w!= \"?\"]\n",
    "W = sorted(list(set(W)))\n",
    "L=sorted(L)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "04eb5d5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "##############################################################\n",
    "# Generate Word Vectors and Stemming of the Given Strings\n",
    "##############################################################\n",
    "\n",
    "def Generate_vector (vocabulary,text):\n",
    "    #####################################################\n",
    "    #####################################################\n",
    "    from nltk.chat.util import Chat, reflections\n",
    "    from nltk.stem import PorterStemmer\n",
    "    from nltk.stem import WordNetLemmatizer\n",
    "    import nltk\n",
    "    import numpy as np\n",
    "    stemmer = PorterStemmer()\n",
    "    lemmatizer=WordNetLemmatizer()\n",
    "    ####################################################\n",
    "    ####################################################\n",
    "    tokens=nltk.word_tokenize(text)\n",
    "    tokens=[word.lower() for word in tokens]\n",
    "    for each_word in tokens:\n",
    "        if each_word in stop_words:\n",
    "            tokens.remove(each_word)\n",
    "    #print(tokens)\n",
    "    pure_tokens=[stemmer.stem(w.lower()) for w in tokens]\n",
    "    length=range(len(vocabulary))\n",
    "    out=[]\n",
    "    for ind in length:\n",
    "        if vocabulary[ind] in pure_tokens:\n",
    "            if vocabulary[ind] not in out:\n",
    "                out.append(1)\n",
    "        else:\n",
    "            out.append(0)\n",
    "    #print(out)\n",
    "    return np.array(out).reshape(1,135)\n",
    "    \n",
    "##############################################################\n",
    "# End of Function\n",
    "##############################################################\n",
    "\n",
    "##############################################################\n",
    "# Embedding Vector One-Hot out\n",
    "##############################################################\n",
    "def word_vector_out(dictionary, labels, doc_x,doc_y=None):\n",
    "    Train=[]\n",
    "    Target=[]\n",
    "    out_empty=[0 for _ in range(len(labels))]\n",
    "    for x, doc in enumerate(doc_x):\n",
    "        bag=[]\n",
    "        w_temp=[stemmer.stem(w.lower()) for w in doc]\n",
    "        for w in dictionary:\n",
    "            if w in w_temp:\n",
    "                bag.append(1)\n",
    "            else:\n",
    "                bag.append(0)\n",
    "        if not doc_y is None:\n",
    "            output_row=out_empty[:]\n",
    "            output_row[labels.index(doc_y[x])]=1\n",
    "            Train.append(bag)\n",
    "            Target.append(output_row)\n",
    "        else:\n",
    "            Train.append(bag)\n",
    "    if not doc_y is None:\n",
    "        return Train,Target\n",
    "    else:\n",
    "        return Train\n",
    "#############################################################\n",
    "# End of Function\n",
    "############################################################\n",
    "X_train,y_train=word_vector_out(W,L,doc_x=doc_x,doc_y=doc_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "aca5d31f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/80\n",
      "4/4 [==============================] - 0s 32ms/step - loss: 0.1092 - categorical_accuracy: 0.1667 - val_loss: 0.1099 - val_categorical_accuracy: 0.0769\n",
      "Epoch 2/80\n",
      "4/4 [==============================] - 0s 6ms/step - loss: 0.1087 - categorical_accuracy: 0.2255 - val_loss: 0.1099 - val_categorical_accuracy: 0.0769\n",
      "Epoch 3/80\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.1099 - categorical_accuracy: 0.1373 - val_loss: 0.1099 - val_categorical_accuracy: 0.0769\n",
      "Epoch 4/80\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.1099 - categorical_accuracy: 0.1471 - val_loss: 0.1099 - val_categorical_accuracy: 0.0769\n",
      "Epoch 5/80\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.1103 - categorical_accuracy: 0.1275 - val_loss: 0.1099 - val_categorical_accuracy: 0.0769\n",
      "Epoch 6/80\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.1096 - categorical_accuracy: 0.1373 - val_loss: 0.1099 - val_categorical_accuracy: 0.0769\n",
      "Epoch 7/80\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.1091 - categorical_accuracy: 0.1176 - val_loss: 0.1099 - val_categorical_accuracy: 0.0769\n",
      "Epoch 8/80\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.1097 - categorical_accuracy: 0.1078 - val_loss: 0.1099 - val_categorical_accuracy: 0.0769\n",
      "Epoch 9/80\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.1090 - categorical_accuracy: 0.1569 - val_loss: 0.1099 - val_categorical_accuracy: 0.0769\n",
      "Epoch 10/80\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.1103 - categorical_accuracy: 0.1176 - val_loss: 0.1099 - val_categorical_accuracy: 0.0769\n",
      "Epoch 11/80\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.1088 - categorical_accuracy: 0.1863 - val_loss: 0.1100 - val_categorical_accuracy: 0.0769\n",
      "Epoch 12/80\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.1099 - categorical_accuracy: 0.1176 - val_loss: 0.1100 - val_categorical_accuracy: 0.0769\n",
      "Epoch 13/80\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.1107 - categorical_accuracy: 0.0980 - val_loss: 0.1100 - val_categorical_accuracy: 0.0769\n",
      "Epoch 14/80\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.1104 - categorical_accuracy: 0.1863 - val_loss: 0.1100 - val_categorical_accuracy: 0.0769\n",
      "Epoch 15/80\n",
      "4/4 [==============================] - ETA: 0s - loss: 0.1080 - categorical_accuracy: 0.12 - 0s 3ms/step - loss: 0.1092 - categorical_accuracy: 0.0980 - val_loss: 0.1100 - val_categorical_accuracy: 0.0769\n",
      "Epoch 16/80\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.1103 - categorical_accuracy: 0.1667 - val_loss: 0.1100 - val_categorical_accuracy: 0.0769\n",
      "Epoch 17/80\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.1104 - categorical_accuracy: 0.1078 - val_loss: 0.1100 - val_categorical_accuracy: 0.0769\n",
      "Epoch 18/80\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.1096 - categorical_accuracy: 0.0980 - val_loss: 0.1100 - val_categorical_accuracy: 0.0769\n",
      "Epoch 19/80\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.1083 - categorical_accuracy: 0.1863 - val_loss: 0.1100 - val_categorical_accuracy: 0.0769\n",
      "Epoch 20/80\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.1094 - categorical_accuracy: 0.1275 - val_loss: 0.1100 - val_categorical_accuracy: 0.0769\n",
      "Epoch 21/80\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.1095 - categorical_accuracy: 0.0980 - val_loss: 0.1100 - val_categorical_accuracy: 0.0769\n",
      "Epoch 22/80\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.1100 - categorical_accuracy: 0.1275 - val_loss: 0.1100 - val_categorical_accuracy: 0.0769\n",
      "Epoch 23/80\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.1105 - categorical_accuracy: 0.1275 - val_loss: 0.1100 - val_categorical_accuracy: 0.0769\n",
      "Epoch 24/80\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.1096 - categorical_accuracy: 0.1569 - val_loss: 0.1101 - val_categorical_accuracy: 0.0769\n",
      "Epoch 25/80\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.1099 - categorical_accuracy: 0.1471 - val_loss: 0.1101 - val_categorical_accuracy: 0.0769\n",
      "Epoch 26/80\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.1099 - categorical_accuracy: 0.1471 - val_loss: 0.1101 - val_categorical_accuracy: 0.0769\n",
      "Epoch 27/80\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.1080 - categorical_accuracy: 0.1863 - val_loss: 0.1101 - val_categorical_accuracy: 0.0385\n",
      "Epoch 28/80\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.1092 - categorical_accuracy: 0.1275 - val_loss: 0.1101 - val_categorical_accuracy: 0.0385\n",
      "Epoch 29/80\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.1099 - categorical_accuracy: 0.1569 - val_loss: 0.1101 - val_categorical_accuracy: 0.0385\n",
      "Epoch 30/80\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.1096 - categorical_accuracy: 0.1569 - val_loss: 0.1101 - val_categorical_accuracy: 0.0385\n",
      "Epoch 31/80\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.1098 - categorical_accuracy: 0.1275 - val_loss: 0.1101 - val_categorical_accuracy: 0.0385\n",
      "Epoch 32/80\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.1096 - categorical_accuracy: 0.1275 - val_loss: 0.1101 - val_categorical_accuracy: 0.0385\n",
      "Epoch 33/80\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.1101 - categorical_accuracy: 0.1471 - val_loss: 0.1101 - val_categorical_accuracy: 0.0385\n",
      "Epoch 34/80\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.1084 - categorical_accuracy: 0.1667 - val_loss: 0.1101 - val_categorical_accuracy: 0.0385\n",
      "Epoch 35/80\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.1097 - categorical_accuracy: 0.1569 - val_loss: 0.1101 - val_categorical_accuracy: 0.0385\n",
      "Epoch 36/80\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.1098 - categorical_accuracy: 0.1176 - val_loss: 0.1101 - val_categorical_accuracy: 0.0385\n",
      "Epoch 37/80\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.1109 - categorical_accuracy: 0.0980 - val_loss: 0.1102 - val_categorical_accuracy: 0.0385\n",
      "Epoch 38/80\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.1097 - categorical_accuracy: 0.1569 - val_loss: 0.1102 - val_categorical_accuracy: 0.0385\n",
      "Epoch 39/80\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.1096 - categorical_accuracy: 0.1569 - val_loss: 0.1102 - val_categorical_accuracy: 0.0385\n",
      "Epoch 40/80\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.1095 - categorical_accuracy: 0.0980 - val_loss: 0.1102 - val_categorical_accuracy: 0.0385\n",
      "Epoch 41/80\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.1093 - categorical_accuracy: 0.1765 - val_loss: 0.1102 - val_categorical_accuracy: 0.0385\n",
      "Epoch 42/80\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.1090 - categorical_accuracy: 0.1667 - val_loss: 0.1102 - val_categorical_accuracy: 0.0385\n",
      "Epoch 43/80\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.1090 - categorical_accuracy: 0.0980 - val_loss: 0.1102 - val_categorical_accuracy: 0.0385\n",
      "Epoch 44/80\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.1091 - categorical_accuracy: 0.1667 - val_loss: 0.1102 - val_categorical_accuracy: 0.0385\n",
      "Epoch 45/80\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.1093 - categorical_accuracy: 0.2451 - val_loss: 0.1102 - val_categorical_accuracy: 0.0385\n",
      "Epoch 46/80\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.1103 - categorical_accuracy: 0.1373 - val_loss: 0.1102 - val_categorical_accuracy: 0.0385\n",
      "Epoch 47/80\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.1099 - categorical_accuracy: 0.1176 - val_loss: 0.1102 - val_categorical_accuracy: 0.0385\n",
      "Epoch 48/80\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.1103 - categorical_accuracy: 0.1667 - val_loss: 0.1102 - val_categorical_accuracy: 0.0385\n",
      "Epoch 49/80\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.1095 - categorical_accuracy: 0.1667 - val_loss: 0.1102 - val_categorical_accuracy: 0.0385\n",
      "Epoch 50/80\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.1093 - categorical_accuracy: 0.1471 - val_loss: 0.1102 - val_categorical_accuracy: 0.0385\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 51/80\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.1090 - categorical_accuracy: 0.1569 - val_loss: 0.1103 - val_categorical_accuracy: 0.0385\n",
      "Epoch 52/80\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.1094 - categorical_accuracy: 0.1275 - val_loss: 0.1103 - val_categorical_accuracy: 0.0385\n",
      "Epoch 53/80\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.1093 - categorical_accuracy: 0.1471 - val_loss: 0.1103 - val_categorical_accuracy: 0.0385\n",
      "Epoch 54/80\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.1094 - categorical_accuracy: 0.1667 - val_loss: 0.1103 - val_categorical_accuracy: 0.0385\n",
      "Epoch 55/80\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.1097 - categorical_accuracy: 0.1176 - val_loss: 0.1103 - val_categorical_accuracy: 0.0385\n",
      "Epoch 56/80\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.1104 - categorical_accuracy: 0.1176 - val_loss: 0.1103 - val_categorical_accuracy: 0.0385\n",
      "Epoch 57/80\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.1085 - categorical_accuracy: 0.1569 - val_loss: 0.1103 - val_categorical_accuracy: 0.0385\n",
      "Epoch 58/80\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.1090 - categorical_accuracy: 0.1275 - val_loss: 0.1103 - val_categorical_accuracy: 0.0385\n",
      "Epoch 59/80\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.1095 - categorical_accuracy: 0.1667 - val_loss: 0.1103 - val_categorical_accuracy: 0.0385\n",
      "Epoch 60/80\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.1089 - categorical_accuracy: 0.2451 - val_loss: 0.1103 - val_categorical_accuracy: 0.0385\n",
      "Epoch 61/80\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.1092 - categorical_accuracy: 0.1569 - val_loss: 0.1103 - val_categorical_accuracy: 0.0385\n",
      "Epoch 62/80\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.1100 - categorical_accuracy: 0.1275 - val_loss: 0.1103 - val_categorical_accuracy: 0.0385\n",
      "Epoch 63/80\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.1096 - categorical_accuracy: 0.1961 - val_loss: 0.1103 - val_categorical_accuracy: 0.0385\n",
      "Epoch 64/80\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.1090 - categorical_accuracy: 0.1961 - val_loss: 0.1104 - val_categorical_accuracy: 0.0385\n",
      "Epoch 65/80\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.1092 - categorical_accuracy: 0.1078 - val_loss: 0.1104 - val_categorical_accuracy: 0.0385\n",
      "Epoch 66/80\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.1096 - categorical_accuracy: 0.1373 - val_loss: 0.1104 - val_categorical_accuracy: 0.0385\n",
      "Epoch 67/80\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.1090 - categorical_accuracy: 0.1863 - val_loss: 0.1104 - val_categorical_accuracy: 0.0385\n",
      "Epoch 68/80\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.1084 - categorical_accuracy: 0.1667 - val_loss: 0.1104 - val_categorical_accuracy: 0.0385\n",
      "Epoch 69/80\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.1090 - categorical_accuracy: 0.2255 - val_loss: 0.1104 - val_categorical_accuracy: 0.0385\n",
      "Epoch 70/80\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.1099 - categorical_accuracy: 0.1176 - val_loss: 0.1104 - val_categorical_accuracy: 0.0385\n",
      "Epoch 71/80\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.1091 - categorical_accuracy: 0.1765 - val_loss: 0.1104 - val_categorical_accuracy: 0.0385\n",
      "Epoch 72/80\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.1096 - categorical_accuracy: 0.1863 - val_loss: 0.1104 - val_categorical_accuracy: 0.0385\n",
      "Epoch 73/80\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.1087 - categorical_accuracy: 0.1863 - val_loss: 0.1104 - val_categorical_accuracy: 0.0385\n",
      "Epoch 74/80\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.1088 - categorical_accuracy: 0.1471 - val_loss: 0.1104 - val_categorical_accuracy: 0.0385\n",
      "Epoch 75/80\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.1097 - categorical_accuracy: 0.1765 - val_loss: 0.1104 - val_categorical_accuracy: 0.0385\n",
      "Epoch 76/80\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.1090 - categorical_accuracy: 0.0980 - val_loss: 0.1104 - val_categorical_accuracy: 0.0385\n",
      "Epoch 77/80\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.1103 - categorical_accuracy: 0.0686 - val_loss: 0.1105 - val_categorical_accuracy: 0.0385\n",
      "Epoch 78/80\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.1093 - categorical_accuracy: 0.1667 - val_loss: 0.1105 - val_categorical_accuracy: 0.0385\n",
      "Epoch 79/80\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.1100 - categorical_accuracy: 0.1667 - val_loss: 0.1105 - val_categorical_accuracy: 0.0385\n",
      "Epoch 80/80\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.1095 - categorical_accuracy: 0.1078 - val_loss: 0.1105 - val_categorical_accuracy: 0.0385\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1d5df6ae670>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from tensorflow.keras import optimizers\n",
    "import tensorflow.keras as k\n",
    "\n",
    "model=Sequential()\n",
    "model.add(Dense(64,input_dim=len(X_train[0]),activation=\"relu\"))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(32,activation=\"relu\"))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(8,activation=\"softmax\"))\n",
    "adam = optimizers.Adam(lr = 0.1)\n",
    "model.compile(optimizer = \"sgd\", loss = 'mse', metrics =k.metrics.CategoricalAccuracy())\n",
    "#print(model.summary())\n",
    "model.fit(np.array(X_train),np.array(y_train),epochs=80,validation_split=0.2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "530943b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy\n",
    "import random\n",
    "\n",
    "##############################################################\n",
    "# The Actual function of the Chat\n",
    "##############################################################\n",
    "def chat2():\n",
    "    print(\"Chat with Great Learning(type : Stop to quit)\")\n",
    "    print(\"If answer is not right type *\")\n",
    "    tag=\"\"\n",
    "    while True:\n",
    "        print(\"----------------------\")\n",
    "        inp=input(\"\\n\\nYou : \").lower()\n",
    "        if inp==\"*\":\n",
    "            print(\"\\n\\nBot : Please rephrase your question and try again\")\n",
    "        if inp==\"quit\":\n",
    "            break\n",
    "        responses=[]\n",
    "        try:\n",
    "            for each in Corpus[\"intents\"]:\n",
    "                search=each[\"patterns\"]\n",
    "                #print(search)\n",
    "                #print(\"----------------------\")\n",
    "                if inp in search:\n",
    "                    #print(\"yay\")\n",
    "                    tag=each[\"tag\"]\n",
    "                    responses=each[\"responses\"]\n",
    "                    raise Exception\n",
    "                    break\n",
    "            #print(\"after try case\")\n",
    "            tokens=Generate_vector(W,inp)\n",
    "            #print(tokens)\n",
    "            results = model.predict(tokens)\n",
    "            #print(\"----------------------\")\n",
    "            #print(results)\n",
    "            results_index = numpy.argmax(results)\n",
    "            #print(\"----------------------\")\n",
    "            #print(results_index)\n",
    "            #print(\"----------------------\")\n",
    "            tag=L[results_index]\n",
    "            #print(tag)\n",
    "            for tg in Corpus[\"intents\"]:\n",
    "                if tg[\"tag\"]==tag:\n",
    "                    responses=tg[\"responses\"]\n",
    "                    raise Exception\n",
    "            if len(responses)==0:   \n",
    "                print(\"Didn't Understand the Question, can you please Re-Phrase it ?\")\n",
    "        except Exception:\n",
    "            if len(responses)!=0:\n",
    "                print(\"Bot : \"+random.choice(responses))\n",
    "                print(\"----------------------\")\n",
    "            if tag==\"Exit\":\n",
    "                break\n",
    "    \n",
    "##############################################################\n",
    "# End of Function\n",
    "##############################################################\n",
    "\n",
    "\n",
    "##############################################################\n",
    "# Check Results Function\n",
    "##############################################################\n",
    "\n",
    "#def \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "##############################################################\n",
    "# End of Function\n",
    "##############################################################\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "bf9b0307",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chat with Great Learning(type : Stop to quit)\n",
      "If answer is not right type *\n",
      "----------------------\n",
      "\n",
      "\n",
      "You : hello\n",
      "Bot : Hello! how can i help you ?\n",
      "----------------------\n",
      "----------------------\n",
      "\n",
      "\n",
      "You : I am not able to understand olympus\n",
      "Bot : Link: Olympus wiki\n",
      "----------------------\n",
      "----------------------\n",
      "\n",
      "\n",
      "You : i am not able to understand knn\n",
      "Bot : Link: Machine Learning wiki \n",
      "----------------------\n",
      "----------------------\n",
      "\n",
      "\n",
      "You : no link visible on olympus\n",
      "Bot : Link: Olympus wiki\n",
      "----------------------\n",
      "----------------------\n",
      "\n",
      "\n",
      "You : unable to understand deep learning\n",
      "Bot : Link: Neural Nets wiki\n",
      "----------------------\n",
      "----------------------\n",
      "\n",
      "\n",
      "You : thank you\n",
      "Bot : I hope I was able to assist you, Good Bye\n",
      "----------------------\n"
     ]
    }
   ],
   "source": [
    "chat2()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee5cad14",
   "metadata": {},
   "source": [
    "# Trials using Rasa\n",
    "\n",
    "I tried using Rasa here, but it is too much of an effort."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "proper-alexandria",
   "metadata": {},
   "outputs": [],
   "source": [
    "from rasa_nlu.training_data import load_data\n",
    "from rasa_nlu.model import Trainer\n",
    "from rasa_nlu import config\n",
    "from rasa_nlu.model import Interpreter\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "valid-relay",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data=load_data(data_json)\n",
    "trainer=Trainer(config.load(config_file))\n",
    "trainer.train(training_data)\n",
    "model_directory=trainer.presist(model_dir,fixed_model_name=\"chat\")\n",
    "interpreter=Interpreter.load(\"chat\")\n",
    "interpreter.parse(input_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "smoking-bicycle",
   "metadata": {},
   "outputs": [],
   "source": [
    "from rasa_core.actions import Action\n",
    "from rasa_core.events import SlotSet\n",
    "\n",
    "user_sign=tracker.get_slot(input_intent)\n",
    "SlotSSet(intent_in_json,)\n",
    "dispatcher.utter_message(intent_in_json[\"response_programmed\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "053bd47d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Sheshank_Joshi\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download(\"punkt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "430fff14",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'tag': 'Intro',\n",
       " 'patterns': ['hi',\n",
       "  'how are you',\n",
       "  'is anyone there',\n",
       "  'hello',\n",
       "  'whats up',\n",
       "  'hey',\n",
       "  'yo',\n",
       "  'listen',\n",
       "  'please help me',\n",
       "  'i am learner from',\n",
       "  'i belong to',\n",
       "  'aiml batch',\n",
       "  'aifl batch',\n",
       "  'i am from',\n",
       "  'my pm is',\n",
       "  'blended',\n",
       "  'online',\n",
       "  'i am from',\n",
       "  'hey ya',\n",
       "  'talking to you for first time'],\n",
       " 'responses': ['Hello! how can i help you ?'],\n",
       " 'context_set': ''}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Corpus[\"intents\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c985bf90",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'tag': 'Exit',\n",
       " 'patterns': ['thank you',\n",
       "  'thanks',\n",
       "  'cya',\n",
       "  'see you',\n",
       "  'later',\n",
       "  'see you later',\n",
       "  'goodbye',\n",
       "  'i am leaving',\n",
       "  'have a Good day',\n",
       "  'you helped me',\n",
       "  'thanks a lot',\n",
       "  'thanks a ton',\n",
       "  'you are the best',\n",
       "  'great help',\n",
       "  'too good',\n",
       "  'you are a good learning buddy'],\n",
       " 'responses': ['I hope I was able to assist you, Good Bye'],\n",
       " 'context_set': ''}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Corpus[\"intents\"][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "2028b1e2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['olympus',\n",
       " 'explain me how olympus works',\n",
       " 'I am not able to understand olympus',\n",
       " 'olympus window not working',\n",
       " 'no access to olympus',\n",
       " 'unable to see link in olympus',\n",
       " 'no link visible on olympus',\n",
       " 'whom to contact for olympus',\n",
       " 'lot of problem with olympus',\n",
       " 'olypus is not a good tool',\n",
       " 'lot of problems with olympus',\n",
       " 'how to use olympus',\n",
       " 'teach me olympus',\n",
       " ['Link: Olympus wiki'],\n",
       " ['Link: Olympus wiki']]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Corpus[\"intents\"][2][\"patterns\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e00d148",
   "metadata": {},
   "source": [
    "### Working Bot copied from some other site. \n",
    "\n",
    "I lost the link where I got it from, but its perfect and uses tkinter. So, it is good for general deployment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "eb3b8f17",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tkinter import *\n",
    "root = Tk()\n",
    "root.title(\"Chatbot\")\n",
    "def send():\n",
    "    send = \"You -> \"+e.get()\n",
    "    txt.insert(END, \"n\"+send)\n",
    "    user = e.get().lower()\n",
    "    if(user == \"hello\"):\n",
    "        txt.insert(END, \"\\n\" + \"Bot -> Hi\")\n",
    "    elif(user == \"hi\" or user == \"hii\" or user == \"hiiii\"):\n",
    "        txt.insert(END, \"n\" + \"Bot -> Hello\")\n",
    "    elif(e.get() == \"how are you\"):\n",
    "        txt.insert(END, \"\\n\" + \"Bot -> fine! and you\")\n",
    "    elif(user == \"fine\" or user == \"i am good\" or user == \"i am doing good\"):\n",
    "        txt.insert(END, \"\\n\" + \"Bot -> Great! how can I help you.\")\n",
    "    else:\n",
    "        txt.insert(END, \"\\n\" + \"Bot -> Sorry! I dind't get you\")\n",
    "    e.delete(0, END)\n",
    "txt = Text(root)\n",
    "txt.grid(row=0, column=0, columnspan=2)\n",
    "e = Entry(root, width=100)\n",
    "e.grid(row=1, column=0)\n",
    "send = Button(root, text=\"Send\", command=send).grid(row=1, column=1)\n",
    "root.mainloop()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf3cce14",
   "metadata": {},
   "source": [
    "# Rule Based Conversations tried my Hand in it\n",
    "\n",
    "https://www.analyticsvidhya.com/blog/2021/07/build-a-simple-chatbot-using-python-and-nltk/\n",
    "\n",
    "Given in the above. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "7550cf6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "pairs = [\n",
    "    [\n",
    "        r\"my name is (.*)\",\n",
    "        [\"Hello %1, How are you today ?\",]\n",
    "    ],\n",
    "    [\n",
    "        r\"hi|hey|hello\",\n",
    "        [\"Hello\", \"Hey there\",]\n",
    "    ], \n",
    "    [\n",
    "        r\"what is your name ?\",\n",
    "        [\"I am a bot created by Analytics Vidhya. you can call me crazy!\",]\n",
    "    ],\n",
    "    [\n",
    "        r\"how are you ?\",\n",
    "        [\"I'm doing goodnHow about You ?\",]\n",
    "    ],\n",
    "    [\n",
    "        r\"sorry (.*)\",\n",
    "        [\"Its alright\",\"Its OK, never mind\",]\n",
    "    ],\n",
    "    [\n",
    "        r\"I am fine\",\n",
    "        [\"Great to hear that, How can I help you?\",]\n",
    "    ],\n",
    "    [\n",
    "        r\"i'm (.*) doing good\",\n",
    "        [\"Nice to hear that\",\"How can I help you?:)\",]\n",
    "    ],\n",
    "    [\n",
    "        r\"(.*) age?\",\n",
    "        [\"I'm a computer program dudenSeriously you are asking me this?\",]\n",
    "    ],\n",
    "    [\n",
    "        r\"what (.*) want ?\",\n",
    "        [\"Make me an offer I can't refuse\",]\n",
    "    ],\n",
    "    [\n",
    "        r\"(.*) created ?\",\n",
    "        [\"Raghav created me using Python's NLTK library \",\"top secret ;)\",]\n",
    "    ],\n",
    "    [\n",
    "        r\"(.*) (location|city) ?\",\n",
    "        ['Indore, Madhya Pradesh',]\n",
    "    ],\n",
    "    [\n",
    "        r\"how is weather in (.*)?\",\n",
    "        [\"Weather in %1 is awesome like always\",\"Too hot man here in %1\",\"Too cold man here in %1\",\"Never even heard about %1\"]\n",
    "    ],\n",
    "    [\n",
    "        r\"i work in (.*)?\",\n",
    "        [\"%1 is an Amazing company, I have heard about it. But they are in huge loss these days.\",]\n",
    "    ],\n",
    "    [\n",
    "        r\"(.*)raining in (.*)\",\n",
    "        [\"No rain since last week here in %2\",\"Damn its raining too much here in %2\"]\n",
    "    ],\n",
    "    [\n",
    "        r\"how (.*) health(.*)\",\n",
    "        [\"I'm a computer program, so I'm always healthy \",]\n",
    "    ],\n",
    "    [\n",
    "        r\"(.*) (sports|game) ?\",\n",
    "        [\"I'm a very big fan of Football\",]\n",
    "    ],\n",
    "    [\n",
    "        r\"who (.*) sportsperson ?\",\n",
    "        [\"Messy\",\"Ronaldo\",\"Roony\"]\n",
    "    ],\n",
    "    [\n",
    "        r\"who (.*) (moviestar|actor)?\",\n",
    "        [\"Brad Pitt\"]\n",
    "    ],\n",
    "    [\n",
    "        r\"i am looking for online guides and courses to learn data science, can you suggest?\",\n",
    "        [\"Crazy_Tech has many great articles with each step explanation along with code, you can explore\"]\n",
    "    ],\n",
    "    [\n",
    "        r\"quit\",\n",
    "        [\"BBye take care. See you soon :) \",\"It was nice talking to you. See you soon :)\"]\n",
    "    ],\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "8be089b1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hi! I am a chatbot created by Analytics Vidhya for your service\n",
      ">hello\n",
      "Hey there\n",
      ">quit\n",
      "It was nice talking to you. See you soon :)\n"
     ]
    }
   ],
   "source": [
    "def chat():\n",
    "    print(\"Hi! I am a chatbot created by Analytics Vidhya for your service\")\n",
    "    chat = Chat(pairs, reflections)\n",
    "    chat.converse()\n",
    "#initiate the conversation\n",
    "if __name__ == \"__main__\":\n",
    "    chat()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc848b15",
   "metadata": {},
   "source": [
    "#### Creating chatbot with the given Corpus in Problem Statement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "5f0cd634",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['thank you', 'thanks', 'cya', 'see you', 'later', 'see you later', 'goodbye', 'i am leaving', 'have a Good day', 'you helped me', 'thanks a lot', 'thanks a ton', 'you are the best', 'great help', 'too good', 'you are a good learning buddy', ['I hope I was able to assist you, Good Bye'], ['I hope I was able to assist you, Good Bye'], ['I hope I was able to assist you, Good Bye']]\n"
     ]
    }
   ],
   "source": [
    "pairs2=[]\n",
    "i=0\n",
    "for each in Corpus[\"intents\"]:\n",
    "    pairs2.append(each[\"patterns\"])\n",
    "    pairs2[i].append(each[\"responses\"])\n",
    "    i+=1\n",
    "print(pairs2[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "9f91c2fc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hi! I am a chatbot created by Great Learning for your service\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "too many values to unpack (expected 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-28-231c3cd42bfc>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;31m#initiate the conversation\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"__main__\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m     \u001b[0mchatbot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-28-231c3cd42bfc>\u001b[0m in \u001b[0;36mchatbot\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mchatbot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Hi! I am a chatbot created by Great Learning for your service\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m     \u001b[0mchat\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mChat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpairs2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreflections\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m     \u001b[0mchat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconverse\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;31m#initiate the conversation\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\nltk\\chat\\util.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, pairs, reflections)\u001b[0m\n\u001b[0;32m     50\u001b[0m         \"\"\"\n\u001b[0;32m     51\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 52\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_pairs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mre\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mre\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mIGNORECASE\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mpairs\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     53\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_reflections\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mreflections\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     54\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_regex\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_compile_reflections\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\nltk\\chat\\util.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     50\u001b[0m         \"\"\"\n\u001b[0;32m     51\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 52\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_pairs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mre\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mre\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mIGNORECASE\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mpairs\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     53\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_reflections\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mreflections\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     54\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_regex\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_compile_reflections\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: too many values to unpack (expected 2)"
     ]
    }
   ],
   "source": [
    "def chatbot():\n",
    "    print(\"Hi! I am a chatbot created by Great Learning for your service\")\n",
    "    chat = Chat(pairs2, reflections)\n",
    "    chat.converse()\n",
    "#initiate the conversation\n",
    "if __name__ == \"__main__\":\n",
    "    chatbot()    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5e21562",
   "metadata": {},
   "source": [
    "Below code is copied from the link given here. Please check it out.\n",
    "\n",
    "https://github.com/ranjiGT/algorithm-adventures/blob/main/chatbot.py\n",
    "\n",
    "After some considerations it is modified to suit for the use here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07eca05a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import nltk\n",
    "import random\n",
    "import string\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "#f = open('C:\\\\Users\\\\User\\\\Downloads\\\\Act1Scene1.txt', 'r', errors='ignore')\n",
    "#raw = f.read()\n",
    "#raw = raw.lower()\n",
    "raw\n",
    "\n",
    "sent_tokens = nltk.sent_tokenize(raw) #converts to list of scentences\n",
    "word_tokens = nltk.word_tokenize(raw) #converts to list of words\n",
    "\n",
    "sentToken = sent_tokens[:4]\n",
    "#print(sentToken)\n",
    "wordToken = word_tokens[:4]\n",
    "#print(wordToken)\n",
    "\n",
    "#preprocessing \n",
    "lemmer = nltk.stem.WordNetLemmatizer()\n",
    "\n",
    "def LemTokens(tokens):\n",
    "    return [lemmer.lemmatize(token) for token in tokens]\n",
    "remove_punct_dict = dict((ord(punct), None) for punct in string.punctuation)\n",
    "\n",
    "def LemNormalize(text):\n",
    "    return LemTokens(nltk.word_tokenize(text.lower().translate(remove_punct_dict)))\n",
    "\n",
    "#Greetings\n",
    "GREETING_INPUTS = (\"hello\", \"hi\", \"greetings\", \"sup\", \"what's up\", \"hey\")\n",
    "GREETING_RESPONSES = [\"hi\", \"hey\", \"nods\", \"hi there\", \"hello\", \"I am glad! you are talking to me\"]\n",
    "\n",
    "def greeting(scentence):\n",
    "    \n",
    "    for word in scentence.split():\n",
    "        if word.lower() in GREETING_INPUTS:\n",
    "            return random.choice(GREETING_RESPONSES)\n",
    "        \n",
    "#Vectorizer\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "def response(user_response):\n",
    "    chatbot_response = ''\n",
    "    sent_tokens.append(user_response)\n",
    "    TfidfVec = TfidfVectorizer(tokenizer=LemNormalize, stop_words=\"english\")\n",
    "    tfidf = TfidfVec.fit_transform(sent_tokens)\n",
    "    vals = cosine_similarity(tfidf[-1], tfidf)\n",
    "    idx = vals.argsort()[0][-2]\n",
    "    flat = vals.flatten()\n",
    "    flat.sort()\n",
    "    req_tfidf = flat[-2]\n",
    "    if(req_tfidf == 0):\n",
    "        chatbot_response = chatbot_response + \"I am sorry! I don't understand you\"\n",
    "        return chatbot_response\n",
    "    \n",
    "    else:\n",
    "        chatbot_response=chatbot_response+sent_tokens[idx]\n",
    "        return chatbot_response\n",
    "    \n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    flag = True\n",
    "    print(\"Hello, there my name is Aneka. I will answer your queries. If you want to exit, type Bye!\")\n",
    "    while(flag==True):\n",
    "        user_response = input()\n",
    "        user_response = user_response.lower()\n",
    "        if(user_response!='bye'):\n",
    "            if user_response == 'thanks' or user_response == 'thank you':\n",
    "                flag = False\n",
    "                print(\"Aneka: You're welcome!\")\n",
    "            else:\n",
    "                if(greeting(user_response)!=None):\n",
    "                    print(\"Aneka:\" +greeting(user_response))\n",
    "                else:\n",
    "                    print(\"Aneka:\", end='')\n",
    "                    print(response(user_response))\n",
    "                    sent_tokens.remove(user_response)\n",
    "        else:\n",
    "            flag = False\n",
    "            print(\"Aneka: Bye! Have a great time!\" )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
